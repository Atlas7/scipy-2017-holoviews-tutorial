{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='http://www.holoviews.org'><img src=\"assets/header_logo.png\" alt=\"HoloViews logo\" width=\"20%;\" align=\"left\"/></a>\n",
    "<div style=\"float:right;\"><h2>07. Working with large datasets</h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import holoviews as hv\n",
    "import dask.dataframe as dd\n",
    "import datashader as ds\n",
    "import geoviews as gv\n",
    "\n",
    "from holoviews.operation.datashader import datashade, aggregate\n",
    "from holoviews.plotting.util import fire\n",
    "datashade.cmap = fire\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>\n",
    "Dask DataFrame is provides a functionally equivalent API to pandas but allows working with data out of core and scales out to many processors and even clusters. Here we will use it to load a large CSV files of taxi coordinates.\n",
    "</p>\n",
    "\n",
    "<div >\n",
    "    <img align=\"left\" src=\"./assets/numba.png\" width='140px'></img>\n",
    "<img align=\"left\" src=\"./assets/dask.svg\" width='115px'></img>\n",
    "<img align=\"left\" src=\"./assets/datashader.png\" width='158px'></img>\n",
    "<img align=\"left\" src=\"./assets/holoviews.png\" width='140px'></img>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load our CSV data with [dask](dask.pydata.org). Dask allows working with datasets larger than fit in memory at one time but also allows parallelizing computations across multiple chunks, which can be processed on multiple threads on your local machine or scaled out to a cluster with 1000s of cores.\n",
    "\n",
    "Dask conveniently provides a DataFrame which does a great job at replicating the pandas DataFrame API. We will load this dataset and persist it, which will load it into memroy, if your machine is low in RAM do not persist the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv('../data/nyc_taxi.csv', parse_dates=['tpep_pickup_datetime'])\n",
    "ddf['hour'] = ddf.tpep_pickup_datetime.dt.hour\n",
    "\n",
    "# If your machine is low on RAM (<8GB) don't persist (everything will be much slower)\n",
    "ddf = ddf.persist()\n",
    "print('%s Rows' % len(ddf))\n",
    "print('Columns:', list(ddf.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous have already seen how to declare a set of [``Points``](http://holoviews.org/reference/elements/bokeh/Points.html) from a DataFrame, this works much the same, we pass in the DataFrame along with the key dimensions. Remember however we have 12 million rows of data, no plotting program will handle this well! Therefore we will use the ``datashader`` operation which will aggregate the data on a 2D grid and then apply shading, leaving us with an ``RGB`` Element to display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%opts RGB [width=800 height=400]\n",
    "points = hv.Points(ddf, kdims=['dropoff_x', 'dropoff_y'])\n",
    "datashade(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you zoom in you will have noted that the plot rerenders depending on the zoom level. This is because the datashade operation is a dynamic operation that also declares some linked streams. These are automatically instantiated and supply the ``x_range`` and ``y_range`` to the operation, which dynamically change as you zoom in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashade.streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Plot pickups\n",
    "\n",
    "* Create a [``Points``](http://holoviews.org/reference/elements/bokeh/Points.html) Element with the 'pickup_x' and 'pickup_y' as key dimensions\n",
    "* Apply the datashade operation to the points\n",
    "* Optional: Change the ``cmap`` to ``inferno``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from colorcet import inferno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a tile source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the GeoViews extension for HoloViews we can display a tile source in the background. Declare a bokeh WMTSTileSource and pass it to the gv.WMTS Element, then we can overlay it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%opts RGB [xaxis=None yaxis=None]\n",
    "import geoviews as gv\n",
    "from bokeh.models import WMTSTileSource\n",
    "url = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{Z}/{Y}/{X}.jpg'\n",
    "wmts = WMTSTileSource(url=url)\n",
    "gv.WMTS(wmts) * datashade(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Overlay on Wikipedia tile source\n",
    "\n",
    "* Create a bokeh ``WMTSTileSource`` with the URL provided below\n",
    "* Overlay the datashaded ``points`` on top of a ``gv.WMTS`` Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://maps.wikimedia.org/osm-intl/{Z}/{X}/{Y}@2x.png'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating with a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have simply been counting taxi dropoffs, but our dataset is much richer than that. We have information about a number of variables including the total cost of a taxi ride, the ``total_amount``. Datashader provides a number of ``aggregator`` functions, which you can supply to the datashade operation. Here use the ``ds.mean`` aggregator to compute the average cost of a trip at a dropoff location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = points.select(total_amount=(None, 1000))\n",
    "selected.data = selected.data.persist()\n",
    "gv.WMTS(wmts) * datashade(selected, aggregator=ds.mean('total_amount'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use an aggregator\n",
    "\n",
    "* Inspect the ``ddf`` dataframe for other variables like 'tip_amount'\n",
    "* Use another aggregator such as ``ds.min`` or ``ds.max``\n",
    "* Generate a datashaded plot of the points with the aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%opts Image [width=600 height=300 logz=True xaxis=None yaxis=None]\n",
    "taxi_ds = hv.Dataset(ddf)\n",
    "grouped = taxi_ds.to(hv.Points, ['dropoff_x', 'dropoff_y'], groupby=['hour'], dynamic=True)\n",
    "aggregate(grouped).redim.values(hour=range(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Facet the data\n",
    "\n",
    "* Reuse the ``taxi_ds`` from above\n",
    "* Select a subset of hours, e.g. in the morning\n",
    "* Group the data by hour using the ``.to`` method\n",
    "* Use a [``NdLayout``](http://holoviews.org/reference/containers/bokeh/GridSpace.html) to facet the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can overlay an invisible ``QuadMesh`` to reveal information on hover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts QuadMesh [width=800 height=400 tools=['hover']] (alpha=0 hover_line_alpha=1 hover_fill_alpha=0)\n",
    "hover_info = aggregate(points, width=40, height=20, streams=[hv.streams.RangeXY]).map(hv.QuadMesh, hv.Image)\n",
    "gv.WMTS(wmts) * datashade(points) * hover_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read more:\n",
    "\n",
    "* Read the user guide on [Working with large data using datashader](http://holoviews.org/user_guide/Large_Data.html)\n",
    "* See a [bokeh app](http://holoviews.org/gallery/apps/bokeh/nytaxi_hover.html) using this dataset and an additional linked stream."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
